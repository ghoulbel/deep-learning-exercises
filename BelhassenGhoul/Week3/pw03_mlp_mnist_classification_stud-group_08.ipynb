{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2744d6",
   "metadata": {},
   "source": [
    "**Group-08**<br/>\n",
    "<font style=\"color:red\"> **Belhassen Ghoul <br/> Robin Ehrensperger <br/> Dominic Diedenhofen**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Load train and test partition of the MNIST dataset.\n",
    "\n",
    "Prepare the training by splitting the training partition into a training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e20431bb-1ece-49f5-a5f6-414a48dcc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into train and validate\n",
    "\n",
    "### YOUR CODE START ###\n",
    "training_set, validation_set = random_split(training_data,[50000,10000])\n",
    "\n",
    "### YOUR CODE END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff404-5b8a-4f63-9730-d6708d2ac8d1",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Implement an MLP model that can be configured with a an arbitrary number of layers and units per layer.\n",
    "\n",
    "To that end, implement a suitable sub-class of `torch.nn.Module` with a constructor that accepts the following arguments:\n",
    "* `units`: list of integers that specify the number of units in the different layers. The first element corresponds to the number of units in the input layer (layer '0'), the last element is the number of output units, i.e. the number of classes the classifier is designed for (10 for an MNIST classifier). Hence, MLP will have $n$ hidden layers if `units` has $n+1$ elements. \n",
    "* `activation_class`: Class name of the activation function layer to be used (such as `torch.nn.ReLU`). Instances can be created by `activation_class()` and added to the succession of layers defined by the model. \n",
    "\n",
    "Alternatively, you can implement a utility method that creates a `torch.nn.Sequential` model accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8e55861c-e424-45b4-845a-48dd576d7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE START ###\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.units = units[1]\n",
    "        self.activation_class = activation_class\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(units[0],self.units)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(self.units,units[2])\n",
    "        self.ReLU2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(units[2],units[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.linear1(self.flatten(x))\n",
    "        z = self.ReLU(z)\n",
    "        z = self.linear2(z)\n",
    "        z = self.ReLU2(z)\n",
    "        return self.linear3(z)\n",
    "        \n",
    "\n",
    "### YOUR CODE END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 300]         235,500\n",
      "              ReLU-3                  [-1, 300]               0\n",
      "            Linear-4                  [-1, 100]          30,100\n",
      "              ReLU-5                  [-1, 100]               0\n",
      "            Linear-6                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.02\n",
      "Estimated Total Size (MB): 1.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28,300, 100, 10])\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff38adb-7cad-4eee-b6ed-34c0e0fd3d07",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For training, implement a method with the arguments:\n",
    "* `model`: Model to be trained\n",
    "* `lr`: Learning rate\n",
    "* `nepochs`: Number of epochs\n",
    "* `batchsize`: Batch size\n",
    "* `training_data`: Training set (subclassed of `Dataset`)\n",
    "* `validation_data`: Validation set (subclassed of `Dataset`)\n",
    "\n",
    "Remember the training and validation cost and accuracy, respectively for monitoring the progress of the training. <br>\n",
    "Note that for the training cost and accuracy you can use the per batch quantities averaged over an epoch. \n",
    "\n",
    "Furthermore, you can use the SGD optimizer of pytorch (`torch.optim.SGD`) - but without momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "172c91d1-4c9e-413a-bfff-01f51a1e323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_set, validation_set):\n",
    "    # finally return the sequence of per epoch values\n",
    "    cost_hist = []\n",
    "    cost_hist_valid = []\n",
    "    acc_hist = []\n",
    "    acc_hist_valid = []\n",
    "\n",
    "    cost_ce = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    ### YOUR CODE START ###\n",
    "    \n",
    "    # epoch: current epoch\n",
    "    # cost, cost_valid, acc, acc_valid: cost and acurracy (for training, validation set) per epoch     \n",
    "    \n",
    "    training_loader = DataLoader(training_set, batch_size=nbatch, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=10000, shuffle=True)\n",
    "\n",
    "    size = len(training_loader.dataset)\n",
    "    nbatches = len(training_loader)\n",
    "\n",
    "    cost, acc = 0.0, 0.0\n",
    "    for epoch in range(nepochs):\n",
    "        for batch, (X, Y) in enumerate(training_loader):\n",
    "\n",
    "            pred = model(X)    \n",
    "            loss= cost_ce(pred,Y)    \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        \n",
    "            acc += (pred.argmax(dim=1)==Y).type(torch.float).sum().item()\n",
    "            cost += cost_ce(pred,Y)\n",
    "\n",
    "        cost /= nbatches\n",
    "        acc /= size\n",
    "    \n",
    "        cost_valid, acc_valid = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X,Y in validation_loader:\n",
    "                pred = model(X)\n",
    "                acc_valid += (pred.argmax(dim=1)==Y).type(torch.float).sum().item()/len(validation_loader.dataset)\n",
    "                cost_valid += cost_ce(pred,Y)\n",
    "\n",
    "        print(\"Epoch %i: %f, %f, %f, %f\"%(epoch, cost, acc, cost_valid, acc_valid))\n",
    "\n",
    "        ### YOUR CODE END ###\n",
    "        \n",
    "        cost_hist.append(cost)\n",
    "        cost_hist_valid.append(cost_valid)\n",
    "        acc_hist.append(acc)\n",
    "        acc_hist_valid.append(acc_valid)\n",
    "    return cost_hist, cost_hist_valid, acc_hist, acc_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "294dc895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.329284, 0.896100, 0.130532, 0.961200\n",
      "Epoch 1: 0.109761, 0.966258, 0.562918, 0.866600\n",
      "Epoch 2: 0.075648, 0.976199, 0.107886, 0.967600\n",
      "Epoch 3: 0.052647, 0.983080, 0.293440, 0.933300\n",
      "Epoch 4: 0.040176, 0.986620, 0.186789, 0.952000\n",
      "Epoch 5: 0.029490, 0.990380, 0.101680, 0.971800\n",
      "Epoch 6: 0.024980, 0.992000, 0.077242, 0.978900\n",
      "Epoch 7: 0.020006, 0.993860, 0.087881, 0.976000\n",
      "Epoch 8: 0.010044, 0.996980, 0.093514, 0.978400\n",
      "Epoch 9: 0.013439, 0.995380, 0.084583, 0.979700\n",
      "Epoch 10: 0.012179, 0.996100, 0.099076, 0.978600\n",
      "Epoch 11: 0.007437, 0.997440, 0.104426, 0.978800\n",
      "Epoch 12: 0.007060, 0.997680, 0.102571, 0.979700\n",
      "Epoch 13: 0.008387, 0.997400, 0.112578, 0.978400\n",
      "Epoch 14: 0.006891, 0.997820, 0.110990, 0.978800\n",
      "Epoch 15: 0.005433, 0.998200, 1.801062, 0.878800\n",
      "Epoch 16: 0.010228, 0.997640, 0.117600, 0.976700\n",
      "Epoch 17: 0.004936, 0.998520, 0.100398, 0.981700\n",
      "Epoch 18: 0.001167, 0.999780, 0.097673, 0.983400\n",
      "Epoch 19: 0.000236, 1.000000, 0.098411, 0.982800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>),\n",
       "  tensor(0.0002, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.1305),\n",
       "  tensor(0.5629),\n",
       "  tensor(0.1079),\n",
       "  tensor(0.2934),\n",
       "  tensor(0.1868),\n",
       "  tensor(0.1017),\n",
       "  tensor(0.0772),\n",
       "  tensor(0.0879),\n",
       "  tensor(0.0935),\n",
       "  tensor(0.0846),\n",
       "  tensor(0.0991),\n",
       "  tensor(0.1044),\n",
       "  tensor(0.1026),\n",
       "  tensor(0.1126),\n",
       "  tensor(0.1110),\n",
       "  tensor(1.8011),\n",
       "  tensor(0.1176),\n",
       "  tensor(0.1004),\n",
       "  tensor(0.0977),\n",
       "  tensor(0.0984)],\n",
       " [0.8961,\n",
       "  0.9662579219999999,\n",
       "  0.97619932515844,\n",
       "  0.9830795239865032,\n",
       "  0.9866196615904796,\n",
       "  0.9903797323932318,\n",
       "  0.9919998075946479,\n",
       "  0.9938598399961519,\n",
       "  0.9969798771967999,\n",
       "  0.9953799395975439,\n",
       "  0.9960999075987919,\n",
       "  0.997439921998152,\n",
       "  0.9976799487984399,\n",
       "  0.997399953598976,\n",
       "  0.997819947999072,\n",
       "  0.9981999563989599,\n",
       "  0.9976399639991279,\n",
       "  0.99851995279928,\n",
       "  0.9997799703990561,\n",
       "  0.999999995599408],\n",
       " [0.9612,\n",
       "  0.8666,\n",
       "  0.9676,\n",
       "  0.9333,\n",
       "  0.952,\n",
       "  0.9718,\n",
       "  0.9789,\n",
       "  0.976,\n",
       "  0.9784,\n",
       "  0.9797,\n",
       "  0.9786,\n",
       "  0.9788,\n",
       "  0.9797,\n",
       "  0.9784,\n",
       "  0.9788,\n",
       "  0.8788,\n",
       "  0.9767,\n",
       "  0.9817,\n",
       "  0.9834,\n",
       "  0.9828])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = (784,10,10)\n",
    "nepochs = 20\n",
    "lr = 0.5\n",
    "\n",
    "train_eval(model,lr=lr,nepochs=nepochs,nbatch=64,training_set=training_set,validation_set=validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b84285-bbd4-4a65-8b8c-1a535f680fff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration\n",
    "\n",
    "Now use this functionality to explore different layer configurations: \n",
    "* Number of layers\n",
    "* Number of units per layer\n",
    "* Suitable learning rate\n",
    "* Suitable number of epochs.\n",
    "\n",
    "Use a batchsize of 64.\n",
    "\n",
    "Make sure that you choose a sufficinetly large number of epochs so that the learning has more or less stabilizes (converged). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c19f6-694f-4447-8ca3-79ef2f8daade",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Summarize your findings with the different settings in a table\n",
    "\n",
    "| Units | nepochs | lr | Acc (Train) | Acc (Valid) |\n",
    "| --- | :-: | :-: | :-: | :-: |\n",
    "| (784,10,10) | 20 | 0.5 | 94.1% | 93.4% |\n",
    "\n",
    "<font style=\"color:red\">the model is much better (98,3%!!!)than the old one with 93% Acc but we have to pay with computation time!</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
