{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c76f8e4",
   "metadata": {},
   "source": [
    "**Group-08**<br/>\n",
    "<font style=\"color:red\"> **Belhassen Ghoul <br/> Robin Ehrensperger <br/> Dominic Diedenhofen**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "7d3c3737-73dc-481a-8cf0-afb3657ca3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de7e72-0814-405b-8c2c-cefa78d66dc0",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "A Model here consists of a succession of layers.\n",
    "\n",
    "Each layer is implemented as a class with the following API-methods:\n",
    "\n",
    "`forward(torch.tensor)`: Computes the forward pass through the layer, i.e. $x\\rightarrow a$<br> and keeps the information needed for computing the backward pass as member variables. \n",
    "`backward(torch.tensor)`: Computes the backward pass through the layer in form of the derivatives, i.e. $da \\rightarrow dx$. On the fly, it also computes the derivatives w.r.t. the parameters of the layer and keeps them as member variables. It assumes that `forward` method has been run before. <br>\n",
    "`update(lr)`: Updates the parameters of the layer in accordance with vanilla gradient descent and scalar learning rate `lr`. It assumes that the `forward` and the `backward`-pass has been run before.  \n",
    "\n",
    "The tensors defined as inputs to the `forward`/`backward`-method are two dimensional with the sample index in the first and the the feature index in the second dimension. \n",
    "\n",
    "For fully connected layers with activation function $s(\\cdot)$ the formulas are given as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdbdc5-d75a-416b-a5df-dfd3ef1fca05",
   "metadata": {},
   "source": [
    "__Forward path:__\n",
    "\n",
    "$X_{i,j}$: Tensor with shape $(n_b,n_x)$ where $n_b$ is the number of samples in the batch and $n_x$ the number of input features (for MNIST: 784).\n",
    "\n",
    "$Z_{i,j} = \\sum_k X_{i,k} W_{j,k} \\qquad (Z = X \\cdot W^T + b)$ $\\qquad$ ($W$ a tensor of shape $(n_h,n_x)$)\n",
    "\n",
    "$A_{i,j} = s(Z_{i,j}) \\qquad\\qquad (A = s(Z))$\n",
    "\n",
    "__Backward path:__ (with $n_b$ the number of samples in a batch)\n",
    "\n",
    "$dx_{i,k} = \\frac{\\partial L}{\\partial x_{i,k}} = \\sum_j \\frac{\\partial L}{\\partial a_{i,j}} \\frac{\\partial a_{i,j}}{\\partial x_{i,k}} = \\sum_j da_{i,j} s^\\prime(z_{i,j})\\cdot \\frac{\\partial z_{i,j}}{\\partial x_{i,k}} = \\sum_j da_{i,j} s^\\prime(z_{i,j}) W_{j,k}$<br>\n",
    "\n",
    "$dW_{k,j} = \\frac{\\partial L}{\\partial W_{k,j}} = \\frac{1}{n_b}\\sum_i da_{i,k}\\frac{\\partial a_{i,k}}{\\partial W_{k,j}} = \\frac{1}{n_b}\\sum_j da_{i,k} s^\\prime(z_{i,k}) \\frac{\\partial z_{i,k}}{\\partial W_{k,j}} = \\frac{1}{n_b}\\sum_j da_{i,k} s^\\prime(z_{i,k}) x_{i,j}$<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad958e2-bb1e-493f-9c4d-40514e20c479",
   "metadata": {},
   "source": [
    "__Parameter Initialisation__ \n",
    "\n",
    "The parameters need to be initialised which will be a topic later in the course. For now use the following rules: \n",
    "* weights normally distributed with mean $0$ and stdev $1/\\sqrt{n_h}$\n",
    "* bias initialized with zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d7916-88e9-4cc9-91e8-ec600741370d",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Important Note on the Implementation</span>\n",
    "\n",
    "Make sure that all the tensors used anywhere in the model components below have `requires_grad=False`.\n",
    "Autograd functionality is not allowed for computing the gradients. - Autograd will be used below for testing whether your implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae0f20-6928-481d-96b2-02cbfa298760",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "c81dc286-9543-4278-a174-6fbba6be026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self, nx, nh):\n",
    "        \"\"\"\n",
    "        nx -- number of input features, i.e. shape of input tensors x given by (*,nb_input)\n",
    "        nh -- number of output features, i.e. shape of output tensor z given by (*,nb_hidden)\n",
    "        \"\"\"    \n",
    "        self.nx = nx\n",
    "        self.nh = nh\n",
    "        self.w = torch.empty(nh, nx).normal_(0, 1./math.sqrt(self.nh))\n",
    "        self.b = torch.zeros(nh)\n",
    "        self.dw = torch.zeros_like(self.w)\n",
    "        self.db = torch.zeros_like(self.b)\n",
    "        self.x = None\n",
    "        self.dx = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the forward pass through the layer\n",
    "        x -- input tensor\n",
    "        returns z \n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        z = torch.matmul(x, self.w.T)+ self.b\n",
    "        return z\n",
    "        ### YOUR CODE END ###\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        \"\"\"\n",
    "        Computes the backward pass through the layer incl. the derivatives w.r.t. input x (dx), weight w (dw) and bias b (db).\n",
    "        dz -- tensor with the backprop'd error signal with the same shape as z.         \n",
    "        returns dx\n",
    "        \"\"\"\n",
    "        assert len(dz.shape)==2 and dz.shape[1] == self.nh\n",
    "        ### YOUR CODE START ###\n",
    "        dx = torch.matmul(dz,self.w)\n",
    "        return dx\n",
    "        ### YOUR CODE END ###\n",
    "            \n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the model (weights w and bias b) with the gradient w.r.t. w and b and learning rate.\n",
    "        returns None\n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        wupdate = torch.matmul(self.z.T, self.dw)\n",
    "        w = w-lr*wupdate\n",
    "        return None        \n",
    "        ### YOUR CODE END ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9fd5b-a7b6-4a74-bd71-ab9bc1c79263",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">SHAPE TEST:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "28d46297-caf7-4b69-bc8f-9b32eb25590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearLayer(3,4)\n",
    "assert (4,3) == linear.w.shape\n",
    "assert (4,)  == linear.b.shape\n",
    "\n",
    "x = torch.tensor([[1.,2,3],[4,5,6]])\n",
    "a = linear.forward(x)\n",
    "assert (2,4) == a.shape\n",
    "\n",
    "dz = torch.tensor([[1.,1,1,1],[2.,2,2,2]])\n",
    "dx = linear.backward(dz)\n",
    "assert (2,3) == dx.shape\n",
    "assert (4,3) == linear.dw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7383-adf3-4822-9d99-644b9eb33e07",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "__Sigmoid__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "ae86ae0a-0382-4349-acb9-6b1e29be36ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SigmoidActivation():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, z):\n",
    "        ### YOUR CODE START ###\n",
    "        sigmoid = 1/(1+torch.exp(-z))\n",
    "        return sigmoid\n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "    def backward(self, da):\n",
    "        ### YOUR CODE START ###\n",
    "        sigmoid = 1/(1+torch.exp(-da))\n",
    "        return sigmoid*(1-sigmoid)\n",
    "        ### YOUR CODE END ###\n",
    "            \n",
    "    def update(self, lr):\n",
    "        ### YOUR CODE START ###\n",
    "        wupdate = torch.matmul(self.z.T, self.dw)\n",
    "        w = w-lr*wupdate\n",
    "        return None           \n",
    "        ### YOUR CODE END ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d949cda-b92b-4c17-a868-73dec9fd1718",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now implement an MLP as a succession of layers - linear layers and non-linear activation layers.\n",
    "For creating an instance, you will pass the following arguments: \n",
    "* nx: number of input features\n",
    "* nunits: list of number of units in the hidden layers including the output layer\n",
    "\n",
    "Add a list of layers as member variable.\n",
    "\n",
    "Use just a linear layer at the end. Further below we will use a CE loss which is based on the finally output logit values (see lecture of week 2) where the softmax probabilities are implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "ee66c47a-d587-47af-8d10-384a157948f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    def __init__(self, nx, nunits):\n",
    "        self.nx = nx\n",
    "        self.nlayers = len(nunits)\n",
    "        self.nunits = nunits\n",
    "        self.nunits.insert(0,nx)\n",
    "        self.nclasses = self.nunits[-1]\n",
    "        self.layers = []\n",
    "        \n",
    "        ### YOUR CODE START ###\n",
    "        # instantiate the different layers (linear and activations)\n",
    "        self.layers.append(torch.nn.Linear(self.nx, self.nlayers))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "        self.layers.append(torch.nn.Linear(self.nlayers,self.nclasses))\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x - input tensor        \n",
    "        returns output tensor of the model\n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        self.x = x\n",
    "        return self.model(x)\n",
    "        ### YOUR CODE END ###\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        dy - derivative w.r.t. output tensor\n",
    "        \n",
    "        returns derivative with respect to the input tensor of the model; \n",
    "        on the fly compute all the derivatives w.r.t. parameters of the model\n",
    "        \"\"\"        \n",
    "        ### YOUR CODE START ###\n",
    "        self.W1 = torch.empty(self.nx,self.nlayers)\n",
    "        self.W2 = torch.empty(self.nlayers,self.nclasses)\n",
    "        \n",
    "        self.z = torch.matmul(self.x,self.W1)\n",
    "        self.z21 = 1/(1+np.exp(-self.z))\n",
    "        self.z2 = self.z21*(1-self.z21)\n",
    "        self.z3= torch.matmul(self.z2,self.W2)\n",
    "        \n",
    "        self.dy=dy\n",
    "        \n",
    "        self.z2_error = torch.matmul(dy,self.W2.T)\n",
    "        self.z2_delta = self.z2_error*self.z2*(1-self.z2)\n",
    "\n",
    "        return self.z2_delta\n",
    "        ### YOUR CODE END ###\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update the parameters with the given (stored) derivatives w.r.t. model parameters by using the given learning rate. \n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        self.zb = torch.matmul(self.z21,self.W2)\n",
    "        self.z2b = 1/(1+np.exp(-self.zb))\n",
    "\n",
    "        E2 = torch.matmul(self.dy, self.W2.T)\n",
    "        dW2 = E2 * self.z21 * (1 - self.z21)\n",
    "\n",
    "        W2_update = torch.matmul(self.z21.T, self.dy) \n",
    "        W1_update = torch.matmul(self.x.T, dW2) \n",
    " \n",
    "        self.W2 = self.W2 - lr * W2_update\n",
    "        self.W1 = self.W1 - lr * W1_update\n",
    "        ### YOUR CODE END ###\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0859db-1d16-4652-8549-b09fa695602b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">SHAPE TEST:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "6020ece6-ccfe-4242-b72c-4d00c0c1f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 2\n",
    "nunits = [3,4]\n",
    "mlp = MLP(nx,nunits)\n",
    "assert 3 == len(mlp.layers)\n",
    "\n",
    "x = torch.tensor([[1.,2],[3,4]])\n",
    "a = mlp.forward(x)\n",
    "assert (2,4) == a.shape\n",
    "\n",
    "da = torch.tensor([[1.,1,1,1],[2.,2,2,2]])\n",
    "dx = mlp.backward(da)\n",
    "assert (2,2) == dx.shape\n",
    "\n",
    "nx = 2\n",
    "nunits = [3,4]\n",
    "mlp = MLP(nx,nunits)\n",
    "assert 3 == len(mlp.layers)\n",
    "\n",
    "x = torch.tensor([[1.,2],[3,4]])\n",
    "a = mlp.forward(x)\n",
    "assert (2,4) == a.shape\n",
    "\n",
    "da = torch.tensor([[1.,1,1,1],[2.,2,2,2]])\n",
    "dx = mlp.backward(da)\n",
    "assert (2,2) == dx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f5f56-97d9-47c3-b0c8-ee3699c2eaca",
   "metadata": {},
   "source": [
    "### Regression Test\n",
    "\n",
    "Create a regression testing that allows you to test your implementation by regressing against the gradients computed by pytorch's autograd.\n",
    "\n",
    "Below you find two functions that may be helpful in \n",
    "1. creating a reference model from the given model - makes sure that in the reference model the exact same initialized parameters are used; furthermore, that teh parameters of the linear layers (w,b) are specified as tensors with `requires_grad=True`. \n",
    "2. comparing the derivatives w.r.t. parameters for model and refmodel. It assumes that for both, model and refmodel, backprop has been executed. For the model, it means that `backward()`has been executed - for the ref model, only `forward` has been executed, but `backward` applied to the output tensor of the refmodel. For the remodel, we use `grad` of the weights and bias tensors, for the model the parameters `dw` and `db` as basis for the comparison.\n",
    "\n",
    "<span style=\"color:red\">Adjust these methods to make them compliant with your model - it uses internals of our implementation.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "3e33f3c4-ab4a-4271-9d5f-39cc9f53ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_refmodel(model):\n",
    "    refmodel = MLP(model.nx, model.nunits[1:])\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if isinstance(layer, LinearLayer):\n",
    "            refmodel.layers[i].w = model.layers[i].w.detach().clone()\n",
    "            refmodel.layers[i].w.requires_grad_()\n",
    "            refmodel.layers[i].b = model.layers[i].b.detach().clone()\n",
    "            refmodel.layers[i].b.requires_grad_()\n",
    "    return refmodel\n",
    "\n",
    "def test_params(model, refmodel, digits=8):\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if isinstance(layer, LinearLayer):\n",
    "            try:\n",
    "                xxref = refmodel.layers[i].w.grad.detach().numpy()\n",
    "                xx = model.layers[i].dw.numpy()\n",
    "                np.testing.assert_array_almost_equal(xx, xxref, decimal=digits, err_msg=\"Error: layer %i\"%i)\n",
    "                xxref = refmodel.layers[i].b.grad.detach().numpy()\n",
    "                xx = model.layers[i].db.numpy()\n",
    "                np.testing.assert_array_almost_equal(xx, xxref, decimal=digits, err_msg=\"Error: layer %i\"%i)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"test failed - reason:\",e) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf6929-9a82-4ec6-94d1-90ccad1752e0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> REGRESSION TEST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "f80d2e20-4fb6-4ed9-ab7f-670d9aca8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "nx = 10\n",
    "x = torch.randn(nx).reshape(-1,nx)\n",
    "\n",
    "# model instance\n",
    "nunits = [20,40,1]\n",
    "mlp = MLP(nx,nunits)\n",
    "\n",
    "# forward and backward pass\n",
    "z = mlp.forward(x)\n",
    "dz = torch.tensor([1.,1.]).reshape(-1,1)\n",
    "dx = mlp.backward(dz)\n",
    "\n",
    "# create ref model\n",
    "mlpref = create_refmodel(mlp)\n",
    "\n",
    "# only use the forward method of the ref model - and apply backward to the output tensor.\n",
    "zref = mlpref.forward(x) \n",
    "zref.backward()\n",
    "\n",
    "# compare the derivatives computed by your model with the grad computed by pytorch's autograd\n",
    "test_params(mlp, mlpref, digits=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d6187-bbbd-4bf1-ae3f-929342eeb39d",
   "metadata": {},
   "source": [
    "### Cost \n",
    "\n",
    "Use the cross-entropy cost function directly defined on the basis of the logits - which implicitly includes a softmax calculation (see lecture notes of week 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f26dbc1b-697b-46cd-94de-d78e701921f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    \n",
    "    def value(self, z, y):\n",
    "        \"\"\"\n",
    "        z -- tensor of shape (number of samples, number of classes) with the final logits of the model. \n",
    "        y -- tensor of shape (number of samples) with the label values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        cost = torch.nn.CrossEntropyLoss()\n",
    "        loss = torch.mean(cost(z,y))*y.shape[0]\n",
    "        return loss\n",
    "\n",
    "        \"\"\"m = y.shape[0]\n",
    "        exp = np.exp(z)   \n",
    "        p = exp/(torch.sum(exp))\n",
    "        log_likelihood = -torch.log(p[range(m),y])\n",
    "        loss = torch.sum(log_likelihood) \n",
    "        return loss\"\"\"\n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "    def derivative(self, z, y):\n",
    "        ### YOUR CODE START ###\n",
    "        softmax = torch.softmax(z,1)\n",
    "\n",
    "        m = y.shape[0]\n",
    "        grad = softmax\n",
    "        grad[range(m),y] -= 1\n",
    "        grad = grad\n",
    "        return grad\n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "9e9e731f-7163-4cc9-a5b0-5c2a000ef8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CELoss()\n",
    "ypred = torch.log(torch.tensor([[0.5,0.4,0.1],[0.2,0.1,0.7]])).reshape(-1,3) # -> logits z\n",
    "y = torch.tensor([1,2]).reshape(-1)\n",
    "np.testing.assert_almost_equal(loss.value(ypred,y), -torch.log(torch.tensor([0.4,0.7])).sum(), decimal=8)\n",
    "np.testing.assert_array_almost_equal(loss.derivative(ypred,y), torch.tensor([[ 0.5000, -0.6000,  0.1000],[ 0.2000,  0.1000, -0.3000]]), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ab724-6516-47ec-9aad-8a2a99a1e78f",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "As in previous' week PW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "da6cb457-1da9-4c14-83c2-345dfe0bf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "b8a76b89-ea2c-4e26-a43d-874ca5b75e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f7fbd-b5fe-432d-b59e-732154aea809",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "Implement mini-batch gradient descent training loop. \n",
    "\n",
    "With the implementation of the two methods below you will be able to train and test the MLP:\n",
    "* train_epoch: for training the model over one epoch with per mini-batch updates\n",
    "* test_epoch: for evaluating the test/validation performance per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "69d69729-6a94-45a6-b6c6-9ecc7d4f2f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loss, dataloader, lr):\n",
    "    \"\"\"\n",
    "    Iterate over the mini-batches of one epoch, compute per mini-batch the forward and backward pass \n",
    "    and update the parameters. Also compute the loss and accuracy as an average over the epoch. \n",
    "    Note that this average includes per mini-batch updated model predictions and parameter updates.\n",
    "    model -- model to be trained\n",
    "    loss -- loss function to be used \n",
    "    dataloader -- data loader that provides mini-batches (from the training set)\n",
    "    lr -- learning rate to be used in the parameter updates     \n",
    "    returns loss, accuracy \n",
    "    \"\"\"\n",
    "    ### YOUR CODE START ###\n",
    "    nsamples = len(dataloader.dataset)\n",
    "    trainloss, correct = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        batchsize = X.shape[0]\n",
    "        X = X.view(batchsize, -1)\n",
    "        z = model.forward(X)\n",
    "        batchloss = loss.value(z, y)\n",
    "        trainloss += batchloss.item()\n",
    "        correct += (z.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        dz = loss.derivative(z,y)\n",
    "        dx = model.backward(dz)\n",
    "        model.update(lr)\n",
    "    trainloss /= nsamples\n",
    "    correct /= nsamples\n",
    "    return trainloss, correct\n",
    "    ### YOUR CODE START ###\n",
    "\n",
    "\n",
    "def test_epoch(model, loss, dataloader):\n",
    "    \"\"\"\n",
    "    Iterate over the mini-batches of one epoch of the test set. Iterates over the mini-batches of the test set.\n",
    "    Estimates loss and accuracy as an average over the test (validation) set. The model is not updates here. \n",
    "    model -- model to be evaluated\n",
    "    loss -- loss function to be evaluated \n",
    "    dataloader -- data loader that provides mini-batches (from the test/validation set)\n",
    "    returns loss, accuracy \n",
    "    \"\"\"\n",
    "    nsamples = len(dataloader.dataset)\n",
    "    testloss, correct = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        batchsize = X.shape[0]\n",
    "        X = X.view(batchsize, -1)\n",
    "        z = model.forward(X)\n",
    "        testloss += loss.value(z, y)\n",
    "        correct += (z.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    testloss /= nsamples\n",
    "    correct /= nsamples\n",
    "    return testloss, correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c7fe4-5c5f-40d4-a7b2-8f650140094f",
   "metadata": {},
   "source": [
    "### First Simple Check: Overfitting on Single Sample\n",
    "\n",
    "Load an arbitrary mini-batch from the training set. Train the model by using just this mini-batch.\n",
    "This is another test for checking whether your implementation is capable of learning something (see remark in week 2 of the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "73f29756-4bba-473e-83e9-17996fe1b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "sample_batch, _ = torch.utils.data.random_split(train_data, [64, 60000-64])\n",
    "train_loader = DataLoader(sample_batch, batch_size=64, shuffle=False) # shuffling not needed since only one batch is used.\n",
    "\n",
    "sample_x,sample_y = next(iter(train_loader))\n",
    "print(sample_x.shape,sample_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "9c9c8561-8484-48b2-9369-075f076fa983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 10, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 20, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 30, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 40, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 50, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 60, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 70, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 80, Train Accuracy: 12.5%, Train Loss: 2.309834\n",
      "Epoch: 90, Train Accuracy: 12.5%, Train Loss: 2.309834\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 1.0\n",
    "mlp = MLP(28*28, [100, 10])\n",
    "mseloss = CELoss()\n",
    "trainlosses = []\n",
    "trainaccs = []\n",
    "for t in range(epochs):\n",
    "    trainloss, trainacc = train_epoch(mlp, mseloss, train_loader, lr)\n",
    "    trainlosses.append(trainloss)\n",
    "    trainaccs.append(trainacc)\n",
    "    if t%10==0:\n",
    "        print(f\"Epoch: {t}, Train Accuracy: {(100*trainacc):>0.1f}%, Train Loss: {trainloss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "2464995e-355c-4562-8619-7dea93e5d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train Accuracy')"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(torch.arange(epochs), trainlosses,\"b-\")\n",
    "plt.title(\"Train Loss\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(torch.arange(epochs), trainaccs,\"b-\")\n",
    "plt.ylim([0.0,1])\n",
    "plt.title(\"Train Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d366251-a20b-4aac-bc33-a4c09607078c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training with all the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "32f164c3-0e81-478d-ac9e-c5770d55075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "8ad2ab4f-b355-4fd3-9fad-7f27d3bd3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Accuracy: 9.9%, Train Loss: 2.439136, Test Accuracy: 10.3%, Test Loss: 2.433651\n",
      "Epoch: 1, Train Accuracy: 9.9%, Train Loss: 2.439136, Test Accuracy: 10.3%, Test Loss: 2.433651\n",
      "Epoch: 2, Train Accuracy: 9.9%, Train Loss: 2.439136, Test Accuracy: 10.3%, Test Loss: 2.433651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Belg\\OneDrive - OST\\Master's\\02_Semester\\TSM_DeLearn\\tsm-delearn\\Exercise\\Week3\\pw03_backprop_stud-group_08.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000034?line=5'>6</a>\u001b[0m trainaccs, testaccs \u001b[39m=\u001b[39m [],[]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000034?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000034?line=8'>9</a>\u001b[0m     trainloss, trainacc \u001b[39m=\u001b[39m train_epoch(mlp, mseloss, train_loader, lr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000034?line=9'>10</a>\u001b[0m     testloss, testacc \u001b[39m=\u001b[39m test_epoch(mlp, mseloss, test_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000034?line=10'>11</a>\u001b[0m     trainlosses\u001b[39m.\u001b[39mappend(trainloss)\n",
      "\u001b[1;32mc:\\Users\\Belg\\OneDrive - OST\\Master's\\02_Semester\\TSM_DeLearn\\tsm-delearn\\Exercise\\Week3\\pw03_backprop_stud-group_08.ipynb Cell 28'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loss, dataloader, lr)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000027?line=12'>13</a>\u001b[0m nsamples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000027?line=13'>14</a>\u001b[0m trainloss, correct \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000027?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000027?line=15'>16</a>\u001b[0m     batchsize \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000027?line=16'>17</a>\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mview(batchsize, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:134\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/datasets/mnist.py?line=130'>131</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/datasets/mnist.py?line=132'>133</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/datasets/mnist.py?line=133'>134</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/datasets/mnist.py?line=135'>136</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/datasets/mnist.py?line=136'>137</a>\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:98\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=89'>90</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=90'>91</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=91'>92</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=96'>97</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/transforms.py?line=97'>98</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:146\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/functional.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/functional.py?line=144'>145</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/functional.py?line=145'>146</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/functional.py?line=146'>147</a>\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torchvision/transforms/functional.py?line=147'>148</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1.0\n",
    "mlp = MLP(28*28, [100,10])\n",
    "mseloss = CELoss()\n",
    "trainlosses, testlosses = [],[]\n",
    "trainaccs, testaccs = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trainloss, trainacc = train_epoch(mlp, mseloss, train_loader, lr)\n",
    "    testloss, testacc = test_epoch(mlp, mseloss, test_loader)\n",
    "    trainlosses.append(trainloss)\n",
    "    testlosses.append(testloss)\n",
    "    trainaccs.append(trainacc)\n",
    "    testaccs.append(testacc)\n",
    "    print(f\"Epoch: {epoch}, Train Accuracy: {(100*trainacc):>0.1f}%, Train Loss: {trainloss:>8f}, Test Accuracy: {(100*testacc):>0.1f}%, Test Loss: {testloss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad1661-b9f4-4048-8196-6b76df0a5925",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Belg\\OneDrive - OST\\Master's\\02_Semester\\TSM_DeLearn\\tsm-delearn\\Exercise\\Week3\\pw03_backprop_stud-group_08.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000035?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000035?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(torch\u001b[39m.\u001b[39marange(epochs), trainlosses,\u001b[39m\"\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000035?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(torch\u001b[39m.\u001b[39;49marange(epochs), testlosses,\u001b[39m\"\u001b[39;49m\u001b[39mr-\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000035?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mCE Loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Belg/OneDrive%20-%20OST/Master%27s/02_Semester/TSM_DeLearn/tsm-delearn/Exercise/Week3/pw03_backprop_stud-group_08.ipynb#ch0000035?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\matplotlib\\pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/pyplot.py?line=2754'>2755</a>\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/pyplot.py?line=2755'>2756</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/pyplot.py?line=2756'>2757</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/pyplot.py?line=2757'>2758</a>\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/pyplot.py?line=2758'>2759</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1389'>1390</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1390'>1391</a>\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1391'>1392</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1628'>1629</a>\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1629'>1630</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1630'>1631</a>\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1631'>1632</a>\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1632'>1633</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_axes.py?line=1633'>1634</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=309'>310</a>\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=310'>311</a>\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=311'>312</a>\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(this, kwargs)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:488\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=485'>486</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(xy) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=486'>487</a>\u001b[0m     x \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=487'>488</a>\u001b[0m     y \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=488'>489</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/axes/_base.py?line=489'>490</a>\u001b[0m     x, y \u001b[39m=\u001b[39m index_of(xy[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:1304\u001b[0m, in \u001b[0;36m_check_1d\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1301'>1302</a>\u001b[0m \u001b[39m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1302'>1303</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1303'>1304</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49matleast_1d(x)\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1304'>1305</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1305'>1306</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1306'>1307</a>\u001b[0m         \u001b[39m# work around\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1307'>1308</a>\u001b[0m         \u001b[39m# https://github.com/pandas-dev/pandas/issues/27775 which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1318'>1319</a>\u001b[0m         \u001b[39m# This code should correctly identify and coerce to a\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/matplotlib/cbook/__init__.py?line=1319'>1320</a>\u001b[0m         \u001b[39m# numpy array all pandas versions.\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36matleast_1d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/numpy/core/shape_base.py?line=62'>63</a>\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/numpy/core/shape_base.py?line=63'>64</a>\u001b[0m \u001b[39mfor\u001b[39;00m ary \u001b[39min\u001b[39;00m arys:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/numpy/core/shape_base.py?line=64'>65</a>\u001b[0m     ary \u001b[39m=\u001b[39m asanyarray(ary)\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/numpy/core/shape_base.py?line=65'>66</a>\u001b[0m     \u001b[39mif\u001b[39;00m ary\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/numpy/core/shape_base.py?line=66'>67</a>\u001b[0m         result \u001b[39m=\u001b[39m ary\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Python\\venv\\lib\\site-packages\\torch\\_tensor.py:678\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/_tensor.py?line=675'>676</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/_tensor.py?line=676'>677</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/_tensor.py?line=677'>678</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/_tensor.py?line=678'>679</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Belg/Python/venv/lib/site-packages/torch/_tensor.py?line=679'>680</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(torch.arange(epochs), trainlosses,\"b-\")\n",
    "plt.plot(torch.arange(epochs), testlosses,\"r-\")\n",
    "plt.title(\"CE Loss\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(torch.arange(epochs), trainaccs,\"b-\")\n",
    "plt.plot(torch.arange(epochs), testaccs,\"r-\")\n",
    "plt.ylim([0.7,1])\n",
    "plt.title(\"Accuracy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
