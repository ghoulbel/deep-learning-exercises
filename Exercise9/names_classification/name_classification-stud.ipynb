{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The names can be found in text files in a src directory, one file per language.\n",
    "\n",
    "In the following you can find some utilities to load the data into pandas data frames. \n",
    "\n",
    "We will restrict to some common European languages. \n",
    "\n",
    "With the given selection, we will identify all the occurring characters and initialize an alphabet.<br>\n",
    "For this alphabet, we will use a one-hot-encoding to map them into a vector space representation. \n",
    "\n",
    "Foresee a suitable character for the end of the word, e.g. 'END'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcdir = 'data/names'\n",
    "languages = [\"English\",\"French\",\"Italian\",\"German\",\"Spanish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data directory\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/French.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/Polish.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Japanese.txt\n",
      "data/names/Russian.txt\n",
      "data/names/Spanish.txt\n",
      "data/names/Irish.txt\n",
      "data/names/English.txt\n",
      "data/names/German.txt\n",
      "data/names/Portuguese.txt\n",
      "data/names/Czech.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/Greek.txt\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(findFiles(os.path.join(srcdir,'*.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "def load_data(srcdir, categories=None):\n",
    "    names_list = []\n",
    "    for filename in findFiles(os.path.join(srcdir,'*.txt')):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        if not categories or category in categories: \n",
    "            names = readLines(filename)\n",
    "            names_list.extend([(name,category) for name in names])\n",
    "    df = pd.DataFrame(names_list)\n",
    "    df.columns = [\"name\",\"lang\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abel</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abraham</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albert</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allard</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name    lang\n",
       "0     Abel  French\n",
       "1  Abraham  French\n",
       "2     Adam  French\n",
       "3   Albert  French\n",
       "4   Allard  French"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The names of the different languages\n",
    "names = load_data(srcdir,categories=languages)\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum name length:  18\n"
     ]
    }
   ],
   "source": [
    "maxlen = np.max([len(name) for name in names.name])\n",
    "print(\"Maximum name length: \", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of alphabet:  74\n",
      "[' ', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Á', 'É', 'ß', 'à', 'á', 'ä', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ñ', 'ò', 'ó', 'ö', 'ù', 'ú', 'ü', 'END']\n",
      "{' ': 0, \"'\": 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53, 'Á': 54, 'É': 55, 'ß': 56, 'à': 57, 'á': 58, 'ä': 59, 'ç': 60, 'è': 61, 'é': 62, 'ê': 63, 'ì': 64, 'í': 65, 'ñ': 66, 'ò': 67, 'ó': 68, 'ö': 69, 'ù': 70, 'ú': 71, 'ü': 72, 'END': 73}\n"
     ]
    }
   ],
   "source": [
    "# Just the letters of the alphabet\n",
    "alphabet = sorted(list(set(''.join([name for name in names.name]))))\n",
    "alphabet.append('END')\n",
    "len_alphabet = len(alphabet)\n",
    "# Dict that maps characters to numbers\n",
    "char_index = dict((c, i) for i, c in enumerate(alphabet))\n",
    "print(\"Size of alphabet: \",len_alphabet)\n",
    "print(alphabet)\n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "English    0.646230\n",
       "French     0.048802\n",
       "German     0.127555\n",
       "Italian    0.124912\n",
       "Spanish    0.052502\n",
       "Name: name, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of names in the data\n",
    "# Percentage of names in that language\n",
    "names.groupby('lang')['name'].count()/len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Representations\n",
    "\n",
    "Now construct the vector representation by using one-hot-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_to_index = {country:index for index,country in enumerate(names.lang.unique())}\n",
    "index_to_language = {index:country for index,country in enumerate(names.lang.unique())}\n",
    "\n",
    "def onehot(i, length):\n",
    "    v = np.zeros(length)\n",
    "    v[i] = 1\n",
    "    return v\n",
    "\n",
    "def name_representation(name, maxlen):\n",
    "    ### START YOUR CODE\n",
    "    name_trunc = str(name)[0:maxlen]\n",
    "    # Create vector of length \"alphabet\"\n",
    "    # vector contains\n",
    "    size = len(char_index)\n",
    "    vector = [onehot(char_index.get(j), size) for j in str(name)]\n",
    "    # fill the rest with \n",
    "    for k in range(0,maxlen - len(str(name))):\n",
    "        vector.append(onehot(char_index['END'], size))\n",
    "    return vector\n",
    "    ### START YOUR CODE\n",
    "\n",
    "def lang_representation(language, language_to_index):\n",
    "    y = np.zeros(len(language_to_index))\n",
    "    y[language_to_index.get(language)]=1\n",
    "    return y\n",
    "\n",
    "def lang_from_output(score):\n",
    "    return index_to_language[np.argmax(score)]\n",
    "\n",
    "def predict(name, model):\n",
    "    score = model.predict(np.array([name_representation(name, maxlen)]))[0]\n",
    "    return lang_from_output(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test\n",
    "\n",
    "Split the data into train/test\n",
    "\n",
    "Shuffle the data\n",
    "\n",
    "Transform the names data into a suitable vector respresentation:\n",
    "* names into numpy arrays of shape (*,maxlen,len_alphabet)\n",
    "* language into numpy array of shape (*,len(languages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name     lang\n",
      "3105       Keys  English\n",
      "2507  Gauntlett  English\n",
      "2729     Hawker  English\n",
      "769     Padovan  Italian\n",
      "3792     Oriley  English\n",
      "----- \n",
      "           name     lang\n",
      "50    Chevrolet   French\n",
      "2947    Jeffrey  English\n",
      "4258    Sargent  English\n",
      "1339     Aldren  English\n",
      "1616       Bray  English\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.2\n",
    "\n",
    "### START YOUR CODE\n",
    "# Shuffle and split names data\n",
    "names = names.sample(frac=1)\n",
    "\n",
    "train = names[:int(round((1-test_split)*len(names), 0))]\n",
    "\n",
    "test = names[int(round((1-test_split)*len(names), 0)) : ]\n",
    "\n",
    "assert len(train) + len(test) == len(names)\n",
    "print(test.head(), end='\\n----- \\n')\n",
    "print(train.head())\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 50       Chevrolet\n",
      "2947       Jeffrey\n",
      "4258       Sargent\n",
      "1339        Aldren\n",
      "1616          Bray\n",
      "           ...    \n",
      "323         Adessi\n",
      "2515          Gell\n",
      "4804    Wheatcroft\n",
      "3756     Odriscoll\n",
      "4978          Aust\n",
      "Name: name, Length: 4541, dtype: object>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000014?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(train\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mhead)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000014?line=6'>7</a>\u001b[0m X_train \u001b[39m=\u001b[39m name_representation( train\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m), maxlen\u001b[39m=\u001b[39mmaxlen )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000014?line=7'>8</a>\u001b[0m Y_train \u001b[39m=\u001b[39m lang_representation( train\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m), language_to_index\u001b[39m=\u001b[39mlanguage_to_index )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000014?line=10'>11</a>\u001b[0m X_test \u001b[39m=\u001b[39m name_representation( test\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m), maxlen\u001b[39m=\u001b[39mmaxlen )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000014?line=11'>12</a>\u001b[0m Y_test \u001b[39m=\u001b[39m lang_representation( test, language_to_index\u001b[39m=\u001b[39mlanguage_to_index )\n",
      "\u001b[1;32m/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb Cell 12'\u001b[0m in \u001b[0;36mlang_representation\u001b[0;34m(language, language_to_index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000011?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlang_representation\u001b[39m(language, language_to_index):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000011?line=22'>23</a>\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(language_to_index))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000011?line=23'>24</a>\u001b[0m     y[language_to_index\u001b[39m.\u001b[39;49mget(language)]\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robin/Projects/deep-learning-exercises/Exercise9/names_classification/name_classification-stud.ipynb#ch0000011?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# Map train and test data into vector space (one-hot-vectors)\n",
    "\n",
    "print(train.get('name').head)\n",
    "\n",
    "X_train = name_representation( train.get('name'), maxlen=maxlen )\n",
    "Y_train = lang_representation( train.get('name'), language_to_index=language_to_index )\n",
    "\n",
    "\n",
    "X_test = name_representation( test.get('name'), maxlen=maxlen )\n",
    "Y_test = lang_representation( test, language_to_index=language_to_index )\n",
    "\n",
    "print(X_test.head())\n",
    "print(Y_test.head())\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, pack the data into a Dataset (e.g. when working with in PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train Model: Single Layer with SimpleRNN\n",
    "\n",
    "Create an RNN consisting of a single layer with a SimpleRNN (keras) and a softmax.\n",
    "\n",
    "Then train the model. Play with different number of hidden units in the layer to obtain a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import SimpleRNN\n",
    "\n",
    "\n",
    "# SimpleRNN, single layer with tf.keras....\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE\n",
    "batch_size=\n",
    "nepochs = \n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Model with several SimpleRNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "model = ...\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling\n",
    "\n",
    "Choose a method to address the class imbalance seen in the given example.\n",
    "- minority resampling \n",
    "- class weights in the loss\n",
    "\n",
    "Implement it and incorporate it in the training.\n",
    "Evaluate the results and compare it with the results obtained with the unbalanced training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE\n",
    "\n",
    "# train...\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "904dc1bdff9d4d7e4be896e8be3a0a26d058166099009e9b4b89e1904816c92a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
